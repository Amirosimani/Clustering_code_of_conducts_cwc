{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " note: type of document is not consistnet - we take what the inst has labeled as the true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, v_measure_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import networkx as nx\n",
    "from scipy.spatial import distance\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='darkgrid')\n",
    "stop = stopwords.words('english')\n",
    "today = dt.today().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = '../data/coe.txt'\n",
    "f = open(in_file, 'r')\n",
    "raw_file = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning\n",
    "raw_file = [line.lower() for line in raw_file]\n",
    "raw_file = [line.strip() for line in raw_file]\n",
    "raw_file = [line for line in raw_file if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata\n",
    "region_indices = [i for i, elem in enumerate(raw_file) if 'region:' in elem]\n",
    "name_indices = [i-1 for i in region_indices]\n",
    "country_indices = [i+1 for i in region_indices]\n",
    "org_indices = [i+2 for i in region_indices]\n",
    "doc_indices = [i+3 for i in region_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_start = [i+4 for i in region_indices]\n",
    "text_end = [i-1 for i in name_indices]\n",
    "text_end = text_end[1:]\n",
    "text_end.append(len(raw_file))\n",
    "text_indices = list(zip(text_start, text_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = ['ORG_NAME', 'ORG_TYPE', 'DOC_TYPE', 'COUNTRY', 'REGION', 'TEXT']\n",
    "df = pd.DataFrame(columns=col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ORG_NAME'] = [raw_file[i] for i in name_indices]\n",
    "\n",
    "df['ORG_TYPE'] = [raw_file[i] for i in org_indices]\n",
    "df['ORG_TYPE'] = df['ORG_TYPE'].str.split(':').str[1]\n",
    "\n",
    "df['DOC_TYPE'] = [raw_file[i] for i in doc_indices]\n",
    "df['DOC_TYPE'] = df['DOC_TYPE'].str.split(':').str[1]\n",
    "\n",
    "df['COUNTRY'] = [raw_file[i] for i in country_indices]\n",
    "df['COUNTRY'] = df['COUNTRY'].str.split(':').str[1]\n",
    "\n",
    "df['REGION'] = [raw_file[i] for i in region_indices]\n",
    "df['REGION'] = df['REGION'].str.split(':').str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TEXT'] = [raw_file[s:e] for s,e in text_indices]\n",
    "df['TEXT'] = df['TEXT'].apply(lambda x: ' '.join(x))\n",
    "# df['TEXT'] = df['TEXT'].apply(lambda x: x.split('.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['REGION'] = df['REGION'].apply(lambda x: x.replace(\"asian-pacific group\", \"asia-pacific group\"))\n",
    "df['ORG_TYPE'] = df['ORG_TYPE'].apply(lambda x: x.replace(\"chemistry-industry\", \"chemistry - industry\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DOC_TYPE\n",
       " code of conduct    74\n",
       " code of ethics     64\n",
       " combined            5\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('DOC_TYPE').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count(col, y_lable, title, df=df):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    g = sns.countplot(y=col, data=df,\n",
    "                      order=df[col].value_counts().index,\n",
    "                      color='k')\n",
    "    plt.xticks(rotation=45)\n",
    "#     g.set(yticklabels=['WEOG', 'Asia-Pacific Group', 'Worldwide', 'Easter European Group', 'African Group', 'GRULAC'])\n",
    "    plt.ylabel(y_lable)\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../result/img/count_{}_{}.jpg'.format(col, today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_count('REGION', \"Type of Organization\", \"Count of Documents per Region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Topic Anlaysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'code', 'conduct', 'business', 'solvay', 'shall',\n",
    "                  'may', 'must', 'braskem', 'sasol', 'petrobra', 'petkim', 'yara', 'corporation',\n",
    "                  'dragon_oil', 'akzonobel'])\n",
    "#TODO: expand the stopword vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Tokenize words and Clean-up text\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(df['TEXT']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Amiros/anaconda/lib/python3.6/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.ldamodel import LdaModel as lda\n",
    "from gensim.models import CoherenceModel\n",
    "import math\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = Phrases(data_words, min_count=5, threshold=10) # higher threshold fewer phrases.\n",
    "trigram = Phrases(bigram[data_words], threshold=10)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word.lower() for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "    Output : term dictionary and Document Term Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    \n",
    "#     tfidf_corpus = models.TfidfModel(corpus)\n",
    "\n",
    "    return dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = prepare_corpus(data_lemmatized)\n",
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7893865b08d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLsiModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Amiros/anaconda/lib/python3.6/site-packages/gensim/models/lsimodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, chunksize, decay, distributed, onepass, power_iters, extra_samples, dtype)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Amiros/anaconda/lib/python3.6/site-packages/gensim/models/lsimodel.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, corpus, chunksize, decay)\u001b[0m\n\u001b[1;32m    485\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'initializing %s workers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preparing a new chunk of documents\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                     \u001b[0mnnz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Amiros/anaconda/lib/python3.6/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mchunkize_serial\u001b[0;34m(iterable, chunksize, as_numpy)\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0mwrapped_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m             \u001b[0mwrapped_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwrapped_chunk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Amiros/anaconda/lib/python3.6/site-packages/gensim/models/tfidfmodel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, bow, eps)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mtermid_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtermid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0mtermid_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtermid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mtf_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(b, id2word=a, num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(processed_text, number_of_topics):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary, corpus = prepare_corpus(processed_text)\n",
    "    # generate LDA model\n",
    "    \n",
    "    lda_model = lda(corpus=corpus,\n",
    "                    id2word=dictionary,\n",
    "                    num_topics=number_of_topics, \n",
    "                    random_state=100,\n",
    "                    chunksize=143,\n",
    "                    passes=20,\n",
    "                    alpha='auto',\n",
    "                    per_word_topics=True)\n",
    "    \n",
    "#     print(lda_model.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lda_model, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/Users/Amiros/anaconda/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0mlencorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'TfidfModel' has no len()",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-91bfc2c7322f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# https://radimrehurek.com/gensim/tut2.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_gensim_lsa_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_lemmatized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-4b81fa682d62>\u001b[0m in \u001b[0;36mcreate_gensim_lsa_model\u001b[0;34m(processed_text, number_of_topics)\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     per_word_topics=True)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#     print(lda_model.print_topics(num_topics=number_of_topics, num_words=words))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Amiros/anaconda/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Amiros/anaconda/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input corpus stream has no len(); counting documents\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             \u001b[0mlencorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlencorpus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LdaModel.update() called with an empty corpus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Amiros/anaconda/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    638\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"input corpus stream has no len(); counting documents\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m             \u001b[0mlencorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlencorpus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LdaModel.update() called with an empty corpus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Amiros/anaconda/lib/python3.6/site-packages/gensim/models/tfidfmodel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, bow, eps)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mtermid_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtermid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m             \u001b[0mtermid_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtermid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mtf_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# https://radimrehurek.com/gensim/tut2.html\n",
    "a = create_gensim_lsa_model(data_lemmatized, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(processed_text, number_of_topics):\n",
    "    \"\"\"\n",
    "    Input   : dictionary : Gensim dictionary\n",
    "              corpus : Gensim corpus\n",
    "              texts : List of input texts\n",
    "              stop : Max num of topics\n",
    "    purpose : Compute c_v coherence for various number of topics\n",
    "    Output  : model_list : List of LSA topic models\n",
    "              coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(2, number_of_topics):\n",
    "        print(\"training LDA for {} topics\".format(num_topics))\n",
    "        # generate LDA model\n",
    "        model, dictionary= create_gensim_lsa_model(processed_text, num_topics)\n",
    "        model_list.append(model)\n",
    "        print(\"calculating coherence score for {} topics \\n\".format(num_topics))\n",
    "        coherencemodel = CoherenceModel(model=model, texts=processed_text, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coh_val = compute_coherence_values(data_lemmatized, 20)\n",
    "# model, dictionary  = create_gensim_lsa_model(data_lemmatized, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(coherence_values, stop):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    # Show graph\n",
    "    x = range(2, stop, 1)\n",
    "    plt.plot(x, coherence_values)\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence score\")\n",
    "#     plt.legend((\"coherence_values\"), loc='best')\n",
    "    plt.xticks(xint)\n",
    "    plt.savefig('../result/img/20181230/topics_coherence_20181225.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(coh_val, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[16]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "from pprint import pprint\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: find dominant topic in each document, use it as a new feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. LSA \n",
    "https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models import LsiModel\n",
    "# from gensim.models.phrases import Phraser, Phrases\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# # TODO: remove numbers, use lemmatizer\n",
    "# # better tokenization: n grams : phrases: abu dhabi [6]\n",
    "# # https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/\n",
    "# def preprocess_data(doc_set):\n",
    "#     \"\"\"\n",
    "#     Input  : docuemnt list\n",
    "#     Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "#     Output : preprocessed text\n",
    "#     \"\"\"\n",
    "#     # initialize regex tokenizer\n",
    "#     tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#     # create English stop words list\n",
    "#     en_stop = set(stopwords.words('english'))\n",
    "#     # Create p_stemmer of class PorterStemmer\n",
    "#     p_stemmer = PorterStemmer()\n",
    "#     # list for tokenized documents in loop\n",
    "#     texts = []\n",
    "#     # loop through document list\n",
    "#     for i in doc_set:\n",
    "#         # clean and tokenize document string\n",
    "#         raw = i.lower()\n",
    "#         tokens = tokenizer.tokenize(raw)\n",
    "#         # remove stop words from tokens\n",
    "#         stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "#         # stem tokens\n",
    "# #         stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "#         # add tokens to list\n",
    "#         texts.append(stopped_tokens)\n",
    "#     return texts\n",
    "\n",
    "# def prepare_corpus(doc_clean):\n",
    "#     \"\"\"\n",
    "#     Input  : clean document\n",
    "#     Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Document Term Matrix\n",
    "#     Output : term dictionary and Document Term Matrix\n",
    "#     \"\"\"\n",
    "#     # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "#     dictionary = corpora.Dictionary(doc_clean)\n",
    "#     # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "#     doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "#     # generate LDA model\n",
    "#     return dictionary,doc_term_matrix\n",
    "\n",
    "# def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "#     \"\"\"\n",
    "#     Input  : clean document, number of topics and number of words associated with each topic\n",
    "#     Purpose: create LSA model using gensim\n",
    "#     Output : return LSA model\n",
    "#     \"\"\"\n",
    "#     dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "#     # generate LSA model\n",
    "#     lsamodel = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "#     print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "#     return lsamodel\n",
    "\n",
    "# def compute_coherence_values(dictionary, doc_term_matrix, doc_clean, stop, start=2, step=3):\n",
    "#     \"\"\"\n",
    "#     Input   : dictionary : Gensim dictionary\n",
    "#               corpus : Gensim corpus\n",
    "#               texts : List of input texts\n",
    "#               stop : Max num of topics\n",
    "#     purpose : Compute c_v coherence for various number of topics\n",
    "#     Output  : model_list : List of LSA topic models\n",
    "#               coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "#     \"\"\"\n",
    "#     coherence_values = []\n",
    "#     model_list = []\n",
    "#     for num_topics in range(start, stop, step):\n",
    "#         # generate LSA model\n",
    "#         model = LsiModel(doc_term_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "#         model_list.append(model)\n",
    "#         coherencemodel = CoherenceModel(model=model, texts=doc_clean, dictionary=dictionary, coherence='c_v')\n",
    "#         coherence_values.append(coherencemodel.get_coherence())\n",
    "#     return model_list, coherence_values\n",
    "\n",
    "# def plot_graph(doc_clean,start, stop, step):\n",
    "#     dictionary,doc_term_matrix=prepare_corpus(doc_clean)\n",
    "#     model_list, coherence_values = compute_coherence_values(dictionary, doc_term_matrix,doc_clean,\n",
    "#                                                             stop, start, step)\n",
    "#     # Show graph\n",
    "#     x = range(start, stop, step)\n",
    "#     plt.plot(x, coherence_values)\n",
    "#     plt.xlabel(\"Number of Topics\")\n",
    "#     plt.ylabel(\"Coherence score\")\n",
    "#     plt.legend((\"coherence_values\"), loc='best')\n",
    "#     plt.savefig('../result/img/number_clusters_20181225.png')\n",
    "#     plt.show()\n",
    "\n",
    "# # LSA Model\n",
    "# number_of_topics=7\n",
    "# words=10\n",
    "# clean_text=preprocess_data(df['TEXT'])\n",
    "# model=create_gensim_lsa_model(clean_text,number_of_topics,words)\n",
    "\n",
    "# plt.figure(figsize=(12,10))\n",
    "# start,stop,step=2,10,1\n",
    "# plot_graph(clean_text,start,stop,step)\n",
    "\n",
    "# model=create_gensim_lsa_model(clean_text,6,words)\n",
    "\n",
    "# # model.print_topics(num_topics=number_of_topics, num_words=words)\n",
    "# model.print_topic(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(optimal_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: use the same preprocessing as gensim\n",
    "#TODO: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_numbers(x):\n",
    "#     clean = ''.join([i for i in x if not i.isdigit()])\n",
    "#     return clean\n",
    "\n",
    "# def lemmatize(df):\n",
    "\n",
    "#     docs = df['TEXT_'].tolist()\n",
    "\n",
    "# #     def token_filter(token):\n",
    "# #         return not (token.is_punct | token.is_space | token.is_stop | len(token.text) <= 4)\n",
    "# # for doc in nlp.pipe(docs):\n",
    "# #     tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
    "# #     filtered_tokens.append(tokens)\n",
    "\n",
    "#     filtered_tokens = []\n",
    "\n",
    "#     for doc in nlp.pipe(docs):\n",
    "#         tokens = [token.lemma_ for token in doc]\n",
    "#         filtered_tokens.append(tokens)\n",
    "        \n",
    "#     lemmatized_doc = [' '.join(x for x in doc) for doc in filtered_tokens] \n",
    "#     df['TEXT_lemma'] = lemmatized_doc\n",
    "#     return df\n",
    "\n",
    "# # def remove_stop_words(x):\n",
    "# #     clean = [i for i in word_tokenize(x) if i not in stop]\n",
    "# #     sent = ' '.join(clean)\n",
    "# #     return sent\n",
    "# # [stop.append(x) for x in ['chemistry', 'chemists', 'chemical', 'chemist', 'chemicals', 'code', 'conduct', 'ethics']]\n",
    "\n",
    "# # def remove_special_char(x):\n",
    "# #     clean = re.sub('\\W+',' ', x)\n",
    "# #     return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove numbers\n",
    "# df['TEXT_'] = df['TEXT'].apply(lambda x: remove_numbers(x))\n",
    "# df = lemmatize(df)\n",
    "\n",
    "# # remove specail characters\n",
    "# # df['TEXT_'] = df['TEXT_'].apply(lambda x: remove_special_char(x))\n",
    "# # df['TEXT_'] = df['TEXT'].apply(lambda x: remove_stop_words(x))\n",
    "\n",
    "# df['TEXT_lemma'] = df['TEXT_lemma'].apply(lambda x: x.replace(\"PRON\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer(encoding='utf-8', strip_accents='unicode', lowercase=True,\n",
    "                             analyzer='word', ngram_range=(1, 5),\n",
    "                             min_df=5, max_df=0.5,\n",
    "                             stop_words=stop_words  # , max_features=200\n",
    "                             )),\n",
    "    ('tfidf', TfidfTransformer(smooth_idf=True))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = text_clf.fit_transform(df['TEXT'])\n",
    "feature_names = text_clf.steps[0][1].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top features for all documents - each document\n",
    "def top_features(vectorizer, tfidf_matrix, row, n):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    feature_names = np.array(vectorizer.get_feature_names())  # get all features\n",
    "    feature_index = np.argsort(tfidf_matrix.toarray())[::-1][row,:n]   # sort descending for the given row\n",
    "    feautres_list = [feature_names[i] for i in feature_index]\n",
    "    \n",
    "    return feautres_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each doc\n",
    "top_features(text_clf.steps[0][1], matrix, 0, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_heatmap(vectorizer, tfidf_matrix, keyword_list):\n",
    "    # matrix to array\n",
    "    bow = tfidf_matrix.toarray()\n",
    "    # get all features\n",
    "    feature_names = vectorizer.steps[0][1].get_feature_names()  # get all features\n",
    "    \n",
    "    # find index of keywrods in the feature space\n",
    "    idx = np.where(np.isin(feature_names, keyword_list))\n",
    "\n",
    "    # construct a dataframe\n",
    "    df_kw = pd.DataFrame(bow[:,list(idx[0])], columns=[keyword_list])\n",
    "    df_kw = df_kw[df_kw != 0].dropna(thresh=1)\n",
    "    df_kw = df_kw.fillna(0)  \n",
    "    \n",
    "    plt.figure(figsize=(5, 12))\n",
    "    sns.heatmap(df_kw, fmt=\"g\", cmap='viridis')\n",
    "    \n",
    "#     return df_kw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = ['chemical weapons', 'chemistry']\n",
    "keyword_heatmap(text_clf, matrix, keyword)\n",
    "# plt.savefig('../result/img/feature_heatmap_20181225.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all docs\n",
    "def get_top_n_words(vectorizer, bow, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \"\"\"\n",
    "    sum_words = bow.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vectorizer.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_n_words(text_clf.steps[0][1], matrix, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.text import TSNEVisualizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_clf.fit_transform(df['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['ORG_TYPE']\n",
    "\n",
    "# Create the visualizer and draw the vectors\n",
    "tsne = TSNEVisualizer(random_state=42)\n",
    "tsne.fit(docs, labels)\n",
    "tsne.poof()\n",
    "# plt.savefig('../result/img/tsne/tsne_org.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['REGION']\n",
    "\n",
    "# Create the visualizer and draw the vectors\n",
    "tsne = TSNEVisualizer(random_state=42)\n",
    "tsne.fit(docs, labels)\n",
    "tsne.poof()\n",
    "# plt.savefig('../result/img/tsne/tsne_region.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['DOC_TYPE']\n",
    "\n",
    "# Create the visualizer and draw the vectors\n",
    "tsne = TSNEVisualizer(random_state=42)\n",
    "tsne.fit(docs, labels)\n",
    "tsne.poof()\n",
    "# plt.savefig('../result/img/tsne/type_org.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = cosine_similarity(matrix, matrix)\n",
    "simdf = pd.DataFrame(sim_matrix)\n",
    "simdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster(m, n_cluster):\n",
    "    rs = 42\n",
    "    # Cluster\n",
    "    sc = SpectralClustering(n_cluster, random_state=rs, affinity='rbf', n_init=100)\n",
    "    sc.fit_predict(m)\n",
    "    \n",
    "    return sc\n",
    "# The algorithm takes the top k eigenvectors of the input matrix corresponding to the largest eigenvalues, then runs the k-mean algorithm on the new matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_map(sc, input_data):\n",
    "    G = nx.from_numpy_matrix(sc.affinity_matrix_)\n",
    "    pos = nx.nx.fruchterman_reingold_layout(G)\n",
    "\n",
    "    p = pd.DataFrame.from_dict(pos, orient='index')\n",
    "    p.columns = ['X', 'Y']\n",
    "    p['CLUSTER'] = sc.labels_ + 1\n",
    "    \n",
    "#     p = pd.concat([p, input_data.iloc[:,:-1]],axis=1)   # wihtou text\n",
    "    p = pd.concat([p, input_data], axis=1)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: which metric?\n",
    "# https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation\n",
    "\n",
    "def evaluate_cluster(df, sim_mat, number_of_clusters, metric):\n",
    "    # assign clusters\n",
    "    result_cluster = create_cluster(sim_mat, number_of_clusters)\n",
    "\n",
    "    # map clusters to X and Y\n",
    "    df_plot = cluster_map(result_cluster, df)\n",
    "    \n",
    "    df_plot['CODE_ORG_TYPE'] = df['ORG_TYPE'].astype('category').cat.codes\n",
    "    df_plot['CODE_DOC_TYPE'] = df['DOC_TYPE'].astype('category').cat.codes\n",
    "    df_plot['CODE_REGION'] = df['REGION'].astype('category').cat.codes\n",
    "\n",
    "    mi_region = metric(list(df_plot['CLUSTER'].values), list(df_plot['CODE_REGION'].values))\n",
    "    mi_org = metric(list(df_plot['CLUSTER'].values), list(df_plot['CODE_ORG_TYPE'].values))\n",
    "    mi_doc = metric(list(df_plot['CLUSTER'].values), list(df_plot['CODE_DOC_TYPE'].values))\n",
    "\n",
    "    \n",
    "    all_dist = [mi_region, mi_org, mi_doc]\n",
    "\n",
    "    return all_dist\n",
    "# score = pd.DataFrame(all_dist, columns=['clusters', 'region', 'org', 'doc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating clusters\n",
    "all_score = []\n",
    "for i in range(15):\n",
    "    cluster_score = evaluate_cluster(df, sim_matrix, i+1, v_measure_score)\n",
    "    cluster_score.append(i)\n",
    "    all_score.append(cluster_score)\n",
    "\n",
    "score_df = pd.DataFrame(all_score, columns=['region', 'org', 'doc', 'clusters'])\n",
    "score_df.to_csv('../result/clusters_score_{}.csv'.format(today))\n",
    "score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "ax = sns.lineplot(x=\"clusters\", y='region', label='Region',\n",
    "                  data=score_df)\n",
    "ax = sns.lineplot(x=\"clusters\", y='org', label='Type of Organization',\n",
    "                  data=score_df)\n",
    "ax = sns.lineplot(x=\"clusters\", y='doc', label='Type of Document',\n",
    "                  data=score_df)\n",
    "plt.xticks(range(0, 20))\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"V Measure Score\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.title(\"Evaluation of Clusters for Each Attribute\")\n",
    "\n",
    "x_max = score_df['clusters'][score_df['org'] == score_df['org'].max()]\n",
    "y_max = score_df['org'].max()\n",
    "plt.vlines(x=x_max, ymin=0, ymax=(1.05 * y_max), linestyles=\"dashed\", linewidth=1)\n",
    "plt.text(1.01 * x_max, 1.02 * y_max,'best performing # of clusters')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../result/img/clustesr_score_{}.jpg\".format(today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best number of clusters\n",
    "score_df['clusters'][score_df['org'] == score_df['org'].max()].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign clusters\n",
    "best_cluster = create_cluster(sim_matrix, 7)\n",
    "\n",
    "# map clusters to X and Y\n",
    "df_plot = cluster_map(best_cluster, df)\n",
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def scatter_plot(df, hue_, save=False):\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "    plt.figure(figsize=(16,10))\n",
    "    ax = sns.scatterplot(x=\"X\", y=\"Y\",\n",
    "                         hue=hue_, style= \"CLUSTER\", s= 100,\n",
    "                         data=df, legend=\"full\")\n",
    "\n",
    "    # ax.set_frame_on(False) #Remove both axes\n",
    "    plt.legend(loc=4)\n",
    "    ax.set_ylabel('')    # remove labels\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_yticks([])   # remove ticks\n",
    "    ax.set_xticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.axis('equal')\n",
    "    if save:\n",
    "        plt.savefig('../result/tfidf_cosine_{}_{}.jpg'.format(hue_, today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(df_plot, hue_='REGION', save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: try doc2vec\n",
    "#https://www.kaggle.com/sgunjan05/document-clustering-using-doc2vec-word2vec\n",
    "#https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
