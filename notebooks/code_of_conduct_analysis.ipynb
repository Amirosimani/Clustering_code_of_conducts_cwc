{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import v_measure_score\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from datetime import datetime as dt\n",
    "today = dt.today().strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "in_file = '../data/code_of_conducts_ethics__all.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    f = open(file_path, 'r')\n",
    "    text = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "    # text cleaning\n",
    "    text = [line.lower() for line in text]\n",
    "    text = [line.strip() for line in text]\n",
    "    text = [line for line in text if line]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file = load_data(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9e615e6ebed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf_cleaned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mplot_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     g = sns.countplot(y=col, data=df,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_text' is not defined"
     ]
    }
   ],
   "source": [
    "def get_documet_attributes(text):\n",
    "    \n",
    "    # get line number for `region`\n",
    "    region_indices = [i for i, t in enumerate(text) if 'region:' in t]\n",
    "    # get line number for `organisation name`, `country`, `type of organisation`, `type of document` \n",
    "    name_indices = [i-1 for i in region_indices]\n",
    "    country_indices = [i+1 for i in region_indices]\n",
    "    org_indices = [i+2 for i in region_indices]\n",
    "    doc_indices = [i+3 for i in region_indices]\n",
    "    # get line number for `text`\n",
    "    text_start = [i+4 for i in region_indices]\n",
    "    text_end = [i-1 for i in name_indices]\n",
    "    text_end = text_end[1:]\n",
    "    text_end.append(len(raw_file))\n",
    "    text_indices = list(zip(text_start, text_end))\n",
    "    \n",
    "    # create a dictionary that holds all the indices\n",
    "    d = {'region': region_indices,\n",
    "        'name': name_indices,\n",
    "        'doc_type': doc_indices,\n",
    "        'country': country_indices,\n",
    "        'org_type': org_indices,\n",
    "        'text': text_indices}\n",
    "    \n",
    "    return d\n",
    "\n",
    "def clean_up(df):\n",
    "    \n",
    "    df['TEXT'] = df['TEXT'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    cols = ['ORG_TYPE', 'DOC_TYPE', 'COUNTRY', 'REGION']\n",
    "    df[cols] = df[cols].apply(lambda x: x.str.split(':').str[1], axis=1)\n",
    "\n",
    "    df['REGION'] = df['REGION'].apply(lambda x: x.replace(\"asian-pacific group\", \"asia-pacific group\"))\n",
    "    df['ORG_TYPE'] = df['ORG_TYPE'].apply(lambda x: x.replace(\"chemistry-industry\", \"chemistry - industry\"))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def conver_raw_text(text):\n",
    "    \n",
    "    # get row index for each attribute\n",
    "    d_index = get_documet_attributes(text)\n",
    "    \n",
    "    col_name = ['ORG_NAME', 'ORG_TYPE', 'DOC_TYPE', 'COUNTRY', 'REGION', 'TEXT']\n",
    "    df = pd.DataFrame(columns=col_name)\n",
    "    \n",
    "    df['ORG_NAME'] = [text[i] for i in d_index['name']]\n",
    "    df['ORG_TYPE'] = [text[i] for i in d_index['org_type']]\n",
    "    df['DOC_TYPE'] = [text[i] for i in d_index['doc_type']]\n",
    "    df['COUNTRY'] = [text[i] for i in d_index['country']]\n",
    "    df['REGION'] = [text[i] for i in d_index['region']]\n",
    "    df['TEXT'] = [raw_file[s:e] for s,e in d_index['text']]\n",
    "    \n",
    "    df_cleaned = clean_up(df)\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "def plot_count(col, df):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    g = sns.countplot(y=col, data=df,\n",
    "                      order=df[col].value_counts().index,\n",
    "                      color='k')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = conver_raw_text(raw_file)\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_count('REGION', df_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_txt(df):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words.extend(['from', 'subject', 're', 'edu', 'use', 'code', 'conduct', 'business', 'solvay', 'shall',\n",
    "                      'may', 'must', 'braskem', 'sasol', 'petrobra', 'petkim', 'yara', 'corporation',\n",
    "                      'dragon_oil', 'akzonobel'])\n",
    "\n",
    "\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(encoding='utf-8', strip_accents='unicode', lowercase=True,\n",
    "                                 analyzer='word', ngram_range=(1, 5),\n",
    "                                 min_df=5, max_df=0.5,\n",
    "                                 stop_words=stop_words  # , max_features=200\n",
    "                                 )),\n",
    "        ('tfidf', TfidfTransformer(smooth_idf=True))\n",
    "    ])\n",
    "    \n",
    "    matrix = text_clf.fit_transform(df['TEXT'])    \n",
    "    return matrix, text_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tfidf, vectorizer = vectorize_txt(df_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Feature Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_features(vectorizer, tfidf_matrix, row, n):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    feature_names = np.array(vectorizer.get_feature_names())  # get all features\n",
    "    feature_index = np.argsort(tfidf_matrix.toarray())[::-1][row,:n]   # sort descending for the given row\n",
    "    feautres_list = [feature_names[i] for i in feature_index]\n",
    "    \n",
    "    return feautres_list\n",
    "\n",
    "def keyword_heatmap(vectorizer, tfidf_matrix, keyword_list):\n",
    "    # matrix to array\n",
    "    bow = tfidf_matrix.toarray()\n",
    "    # get all features\n",
    "    feature_names = vectorizer.steps[0][1].get_feature_names()  # get all features\n",
    "    \n",
    "    # find index of keywrods in the feature space\n",
    "    idx = np.where(np.isin(feature_names, keyword_list))\n",
    "\n",
    "    # construct a dataframe\n",
    "    df_kw = pd.DataFrame(bow[:,list(idx[0])], columns=[keyword_list])\n",
    "    df_kw = df_kw[df_kw != 0].dropna(thresh=1)\n",
    "    df_kw = df_kw.fillna(0)  \n",
    "    \n",
    "    plt.figure(figsize=(5, 12))\n",
    "    sns.heatmap(df_kw, fmt=\"g\", cmap='viridis')\n",
    "    \n",
    "def get_top_n_words(vectorizer, bow, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \"\"\"\n",
    "    sum_words = bow.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vectorizer.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each doc\n",
    "top_features(vectorizer.steps[0][1], text_tfidf, 0, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = ['chemical weapons', 'chemistry']\n",
    "keyword_heatmap(vectorizer, text_tfidf, keyword)\n",
    "plt.savefig('../result/img/feature_heatmap_[cw, chemistry]_{}.png'.format(today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_n_words(vectorizer.steps[0][1], text_tfidf, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcualte_similarity(vect):\n",
    "    sim = cosine_similarity(vect, vect)\n",
    "    sim_df = pd.DataFrame(sim)\n",
    "    return sim_df\n",
    "\n",
    "def create_cluster(m, n_cluster):\n",
    "    # The algorithm takes the top k eigenvectors of the input matrix corresponding to the largest eigenvalues, then runs the k-mean algorithm on the new matrix.\n",
    "    # Cluster\n",
    "    sc = SpectralClustering(n_cluster, \n",
    "                            random_state=seed,\n",
    "                            affinity='rbf', \n",
    "                            n_init=100)\n",
    "    sc.fit_predict(m)\n",
    "    \n",
    "    return sc\n",
    "\n",
    "def cluster_map(sc, input_data):\n",
    "    G = nx.from_numpy_matrix(sc.affinity_matrix_)\n",
    "    pos = nx.nx.fruchterman_reingold_layout(G, scale=4)\n",
    "\n",
    "    p = pd.DataFrame.from_dict(pos, orient='index')\n",
    "    p.columns = ['X', 'Y']\n",
    "    p['CLUSTER'] = sc.labels_ + 1\n",
    "    \n",
    "    p = pd.concat([p, input_data], axis=1)\n",
    "    return p\n",
    "\n",
    "def evaluate_cluster(df, sim_mat, number_of_clusters, metric):\n",
    "    # assign clusters\n",
    "    result_cluster = create_cluster(sim_mat, number_of_clusters)\n",
    "\n",
    "    # map clusters to X and Y\n",
    "    df_plot = cluster_map(result_cluster, df)\n",
    "    \n",
    "    df_plot['CODE_ORG_TYPE'] = df['ORG_TYPE'].astype('category').cat.codes\n",
    "    df_plot['CODE_DOC_TYPE'] = df['DOC_TYPE'].astype('category').cat.codes\n",
    "    df_plot['CODE_REGION'] = df['REGION'].astype('category').cat.codes\n",
    "\n",
    "    mi_region = metric(list(df_plot['CLUSTER'].values), list(df_plot['CODE_REGION'].values))\n",
    "    mi_org = metric(list(df_plot['CLUSTER'].values), list(df_plot['CODE_ORG_TYPE'].values))\n",
    "    mi_doc = metric(list(df_plot['CLUSTER'].values), list(df_plot['CODE_DOC_TYPE'].values))\n",
    "\n",
    "    \n",
    "    all_dist = [mi_region, mi_org, mi_doc]\n",
    "\n",
    "    return all_dist\n",
    "\n",
    "def plot_v_score(df, save=False):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    paper_rc = {'lines.linewidth': 5}                  \n",
    "    sns.set_context(\"paper\", rc = paper_rc)   \n",
    "\n",
    "    ax = sns.lineplot(x=\"clusters\", y='region', label='Region',\n",
    "                      data=df)\n",
    "    ax = sns.lineplot(x=\"clusters\", y='org', label='Type of Organization',\n",
    "                      data=df)\n",
    "    ax = sns.lineplot(x=\"clusters\", y='doc', label='Type of Document',\n",
    "                      data=df)\n",
    "    plt.xticks(range(0, 20))\n",
    "    plt.xlabel(\"Number of Clusters\", size=20)\n",
    "    plt.ylabel(\"V Measure Score\",size=20)\n",
    "    # plt.legend(bbox_to_anchor=(1.05, 1), loc=1, borderaxespad=0.)\n",
    "    plt.legend(loc='upper left', borderaxespad=0.2, prop={'size': 20})\n",
    "\n",
    "    plt.title(\"Evaluation of Clusters for Each Attribute\", size=30)\n",
    "\n",
    "\n",
    "    x_max = df['clusters'][df['org'] == df['org'].max()]\n",
    "    y_max = df['org'].max()\n",
    "    plt.vlines(x=x_max, ymin=0, ymax=(1.05 * y_max), linestyles=\"dashed\", linewidth=1)\n",
    "    plt.text(1.01 * x_max, 1.02 * y_max,'Best performing # of clusters', size= 20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(\"../result/img/clustesr_score_{}.jpg\".format(today))\n",
    "        \n",
    "def scatter_plot(df, hue_, save=False):\n",
    "#     sns.set(style=\"white\")\n",
    "    sns.set_palette(\"colorblind\", 10)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(20,14))\n",
    "    ax = sns.scatterplot(x=\"X\", y=\"Y\",\n",
    "                         hue=\"CLUSTER\", style= hue_, s= 200,\n",
    "                         data=df, legend=\"full\", palette=sns.color_palette(\"colorblind\", 7)\n",
    "                         )\n",
    "\n",
    "    # ax.set_frame_on(False) #Remove both axes\n",
    "    plt.legend(loc='upper left', prop={'size': 20}, markerscale=2)\n",
    "    ax.set_ylabel('')    # remove labels\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_yticks([])   # remove ticks\n",
    "    ax.set_xticks([])\n",
    "    plt.title(\"Document Clusters\", size=25)\n",
    "    plt.tight_layout()\n",
    "    plt.axis('equal')\n",
    "    if save:\n",
    "        plt.savefig('../result/img/tfidf_cosine_{}_{}.jpg'.format(hue_, today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = calcualte_similarity(text_tfidf)\n",
    "\n",
    "# evaluating clusters\n",
    "all_score = []\n",
    "for i in range(15):\n",
    "    cluster_score = evaluate_cluster(df_text, sim_matrix, i+1, v_measure_score)\n",
    "    cluster_score.append(i)\n",
    "    all_score.append(cluster_score)\n",
    "\n",
    "score_df = pd.DataFrame(all_score, columns=['region', 'org', 'doc', 'clusters'])\n",
    "score_df.to_csv('../result/clusters_score_{}.csv'.format(today))\n",
    "score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_v_score(score_df, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best number of clusters\n",
    "cluster_num = score_df['clusters'][score_df['org'] == score_df['org'].max()].values[0]\n",
    "\n",
    "# assign clusters\n",
    "best_cluster = create_cluster(sim_matrix, cluster_num)\n",
    "\n",
    "# map clusters to X and Y\n",
    "df_plot = cluster_map(best_cluster, df_text)\n",
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter_plot(df_plot, hue_='ORG_TYPE', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: comment, pep8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
